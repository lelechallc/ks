ada_decay_rate: 0.99999
ada_epsilon: 1e-8
avg_decay_rate: 0.999993
back_propagate_input: true
embedding:
  create_threshold: 0
  initial_g2sum: 3
  initial_range: 2
  learning_rate: 0.05
  slots_config:
  - compress_group: USER
    dim: 4
    dtype: float32
    input_name: user_embedding
    slots: 101 102
  - compress_group: USER
    dim: 64
    dtype: float32
    input_name: photo_embedding
    slots: 103 104
  - dim: 64
    dtype: float32
    input_name: c_id_embedding
    slots: 201 202
  - dim: 32
    dtype: float32
    input_name: c_info_embedding
    slots: 203 204 205 206 207 208
  version_aware: true
  weight_bounds:
  - -10
  - 10
extra_params: []
extra_preds: expand_xtr like_xtr reply_xtr
global_init_range: 0.2
graph: ../config/graph.pb
graph_targets:
- opt
graph_tensor_mapping:
  expand_xtr: Reshape_1:0
  like_xtr: Reshape_2:0
  reply_xtr: Reshape_3:0
hyperparameter_kconf: null
input_type: 3
is_train_placeholder: is_train:0
learning_rate: 5.0e-06
mom_decay_rate: 0.999
output_hidden_units: update_input continue_normlized_input slide_embedding slide_wide
  watch_time_wide position_weight_input
param:
- coln: 128
  dtype: float32
  init_mean: 0
  init_range: 1
  name: multi_head_attention/dense/kernel
  rown: 456
  scale_by_rown: true
- coln: 1
  dtype: float32
  init_mean: 0
  init_range: 0
  name: multi_head_attention/dense/bias
  rank: 1
  rown: 128
  scale_by_rown: true
- coln: 128
  dtype: float32
  init_mean: 0
  init_range: 1
  name: multi_head_attention/dense_1/kernel
  rown: 456
  scale_by_rown: true
- coln: 1
  dtype: float32
  init_mean: 0
  init_range: 0
  name: multi_head_attention/dense_1/bias
  rank: 1
  rown: 128
  scale_by_rown: true
- coln: 128
  dtype: float32
  init_mean: 0
  init_range: 1
  name: multi_head_attention/dense_2/kernel
  rown: 456
  scale_by_rown: true
- coln: 1
  dtype: float32
  init_mean: 0
  init_range: 0
  name: multi_head_attention/dense_2/bias
  rank: 1
  rown: 128
  scale_by_rown: true
- coln: 1
  dtype: float32
  init_mean: 1
  init_range: 0
  name: multi_head_attention/multi_head_attention_norm/gamma_multi_head_attention_norm
  rank: 1
  rown: 128
  scale_by_rown: true
- coln: 1
  dtype: float32
  init_mean: 1
  init_range: 0
  name: multi_head_attention/multi_head_attention_norm/beta_multi_head_attention_norm
  rank: 1
  rown: 128
  scale_by_rown: true
- coln: 1
  dtype: float32
  init_mean: 0
  init_range: 1
  name: expand_xtr/dense/kernel
  rown: 128
  scale_by_rown: true
- coln: 1
  dtype: float32
  init_mean: 0
  init_range: 0
  name: expand_xtr/dense/bias
  rank: 1
  rown: 1
  scale_by_rown: true
- coln: 1
  dtype: float32
  init_mean: 0
  init_range: 1
  name: like_xtr/dense/kernel
  rown: 128
  scale_by_rown: true
- coln: 1
  dtype: float32
  init_mean: 0
  init_range: 0
  name: like_xtr/dense/bias
  rank: 1
  rown: 1
  scale_by_rown: true
- coln: 1
  dtype: float32
  init_mean: 0
  init_range: 1
  name: reply_xtr/dense/kernel
  rown: 128
  scale_by_rown: true
- coln: 1
  dtype: float32
  init_mean: 0
  init_range: 0
  name: reply_xtr/dense/bias
  rank: 1
  rown: 1
  scale_by_rown: true
q_names: expand_xtr like_xtr reply_xtr
summary:
- len: 1912
  name: base_summary
- len: 2072
  name: stub_summary
summary_decay_rate: 0.999999
summary_init_n: 1e4
summary_init_squared_sum: 1e4
summary_squared_sum_epsilon: 1e-4
test_layers_at_joining: join join_forward update update_forward
test_layers_at_updating: update update_forward
test_mode: false
train_layers_at_joining: join
train_layers_at_updating: update
vec_input:
- common: false
  dim: 1
  name: mask_pack
